# threads_parser.py
"""
Daily Threads scraper:
‚Äî –≤—ã—Ç—è–≥–∏–≤–∞–µ—Ç —Å–≤–µ–∂–∏–µ –ø–æ—Å—Ç—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º,
‚Äî –æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ ¬´–∑–∞–ª–µ—Ç–µ–≤—à–∏–µ¬ª (–º–µ—Ç—Ä–∏–∫–∏ –∏ –ø–æ—Ä–æ–≥–∏ –Ω–∏–∂–µ),
‚Äî —à–ª—ë—Ç —Å–ø–∏—Å–æ–∫ –≤–∞–º –≤ Telegram.
–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –¥–æ–±–∞–≤–ª—è–µ—Ç –∫—Ä–∞—Ç–∫—É—é GPT-3.5 –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é ¬´–ø–æ—á–µ–º—É –ø–æ—Å—Ç –∑–∞—à—ë–ª¬ª.
"""

import os
import datetime
import pandas as pd
from threads_client import Threads
from telegram import Bot
from telegram.constants import ParseMode

# --- –ù–ê–°–¢–†–û–ô–ö–ò -------------------------------------------
KEYWORDS = [
    "marketing", "business", "psychology",
    "growth", "founder", "humor marketing", "copywriting"
]
MIN_VIEWS = 40_000      # ‚â• 40 000 –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤
LIKE_RATIO = 0.30       # –ª–∞–π–∫–æ–≤ ‚â• 30 %
COMMENT_RATIO = 0.15    # –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ ‚â• 15 %
MAX_POSTS = 300         # —Å–∫–æ–ª—å–∫–æ –ø–æ—Å—Ç–æ–≤ –∑–∞–ø—Ä–∞—à–∏–≤–∞—Ç—å –Ω–∞ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
# ----------------------------------------------------------

USE_LLM_SUMMARY = bool(os.getenv("OPENAI_API_KEY"))


def fetch_posts() -> list[dict]:
    """–ë–µ—Ä—ë–º –ø–æ—Å—Ç—ã —á–µ—Ä–µ–∑ –Ω–µ–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π Threads-API."""
    th = Threads(os.getenv("THREADS_USERNAME"), os.getenv("THREADS_PASSWORD"))
    all_posts = []
    for kw in KEYWORDS:
        posts = th.search_posts(query=kw, limit=MAX_POSTS)
        for p in posts:
            if p["view_count"] is None:      # –∏–Ω–æ–≥–¥–∞ —Å–∫—Ä—ã—Ç–æ
                continue
            p["keyword"] = kw
            all_posts.append(p)
    return all_posts


def filter_posts(posts: list[dict]) -> pd.DataFrame:
    df = pd.DataFrame(posts)
    df = df[
        (df.view_count >= MIN_VIEWS)
        & (df.like_count / df.view_count >= LIKE_RATIO)
        & (df.reply_count / df.view_count >= COMMENT_RATIO)
    ]
    return df.sort_values("view_count", ascending=False)


def build_message(df: pd.DataFrame) -> str:
    lines = [f"üî• *Top Threads* ‚Äî {datetime.date.today():%d %b %Y}"]
    for _, row in df.head(10).iterrows():
        url = f"https://www.threads.net/t/{row['code']}"
        ratios = (
            f"üëç{row.like_count:,}  üí¨{row.reply_count:,}  üëÄ{row.view_count:,}"
        )
        lines.append(f"‚Ä¢ [{row['keyword']}] {ratios}\n{url}")
    return "\n\n".join(lines)


def send_telegram(text: str) -> None:
    bot = Bot(token=os.getenv("TELEGRAM_BOT_TOKEN"))
    bot.send_message(
        chat_id=os.getenv("TELEGRAM_CHAT_ID"),
        text=text,
        parse_mode=ParseMode.MARKDOWN,
        disable_web_page_preview=True,
    )


def main():
    posts = fetch_posts()
    df = filter_posts(posts)
    if df.empty:
        print("No viral posts today üò¥")
        return

    send_telegram(build_message(df))

    # ---- –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è GPT-–∞–Ω–Ω–æ—Ç–∞—Ü–∏—è ----------------------
    if USE_LLM_SUMMARY:
        from openai import OpenAI

        client = OpenAI()
        prompt = (
            "In one tweet-style paragraph, explain why these Threads posts "
            "went viral. Focus on hooks, emotion and clarity.\n\n"
            + "\n".join(df.text.head(3))
        )
        summary = (
            client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=80,
            )
            .choices[0]
            .message.content.strip()
        )
        send_telegram("üß† *Why it works*\n\n" + summary)


if __name__ == "__main__":
    main()
